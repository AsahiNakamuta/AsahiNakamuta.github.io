<!DOCTYPE html>
<html lang="ja">

<head>
 <meta charset="UTF-8">
 <meta name="viewport" content="width=device-width, initial-scale=1.0">
 <meta name="description" content="執筆中">
 <title>動的平均場理論入門1 - Asahi Nakamuta</title>
 <link rel="stylesheet" href="../css/reset.css">
 <link rel="stylesheet" href="../css/common.css">
 <link rel="stylesheet" href="../css/each.css">
 <link rel="stylesheet"
  href="https://maxst.icons8.com/vue-static/landings/line-awesome/line-awesome/1.3.0/css/line-awesome.min.css">
 <script src="../js/loadCommon.js"></script>
 <style>
    details {
      margin: 10px 0;
    }
    details > *:not(summary) {
        padding-left: 20px;
      }
    details[open] {
        background-color: #f5f5f5; /* 背景色（うすいグレー） */
        padding: 10px;             /* 内側の余白 */
        border-radius: 5px;        /* 角を少し丸くする */
      }
 </style>
    <script>
    document.addEventListener('DOMContentLoaded', () => {
       // .close-btn を持つ要素すべてに対して処理
        document.querySelectorAll('.close-btn').forEach(btn => {
          btn.addEventListener('click', function() {
            // 1. 親のdetails要素を探す
            const details = this.closest('details');
            
            // 2. その中のsummary要素（見出し）を探す
            const summary = details.querySelector('summary');
      
            // 3. detailsを閉じる
            details.removeAttribute('open');
      
            // 4. summaryの位置までスクロールして戻る
            summary.scrollIntoView({
              behavior: 'smooth', // ヌルっと動くアニメーション
              block: 'center'     // 見出しを「画面の中央」に持ってくる
            });
          });
        });
    });
    </script>
</head>

<body>
    <nav id="js-globalnav" class="globalnav" aria-label="メインナビゲーション">
        <ul style="list-style-type:none; padding-inline-start:0;" class="globalnav__main">
            <li class="globalnav__item"><a href="../index.html">トップ</a></li>
            <li class="globalnav__item"><a href="../cv.html">CV</a></li>
            <li class="globalnav__item"><a href="../topics.html">記事一覧</a></li>
        </ul>
    </nav>

 <main class="article" id="js-main">
    <h1 class="heading-m">動的平均場理論入門1</h1>

    <h3>概要</h3>
    <p class="summary">
        執筆中
    </p>

    <h3>論文へのリンク</h3>
    <p>
      <a href="https://arxiv.org/abs/2205.09653" target="_blank">Bordelon & Pehlevan, Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks (2022)</a>
    </p>

    <h3>問題設定</h3>
    <p>
        次のような深層ニューラルネットワークを考えよう。
        \begin{align}
            f_\mu &= \frac{1}{\gamma\sqrt{N}} \boldsymbol{w}^L \cdot \phi(\boldsymbol{h}_\mu^L) \\
            \boldsymbol{h}_\mu^{l+1} &= \frac{1}{\sqrt{N}} \boldsymbol{W}^l \phi(\boldsymbol{h}_\mu^l) \\
            \boldsymbol{h}_\mu^1 &= \frac{1}{\sqrt{D}} \boldsymbol{W}^0 \boldsymbol{x}_\mu
        \end{align}
        ここで $\{\boldsymbol{x}_\mu\}_{\mu\in[P]}$ は $D$ 次元の入力データ、$\boldsymbol{h}^l_\mu$ は各データに対する第 $l$ 層のニューロンの $N$ 次元pre-activationベクトル、$f_\mu$ は最終層のスカラー出力、
        $\boldsymbol{W}^l$ は第 $l$ 層の重み行列、$\phi$ は活性化関数、$\gamma$ は学習のレジームを調整するハイパーパラメータである。
    </p>
    <p>
        このネットワークをフルバッチの勾配降下法で学習する。
        すなわち誤差関数 $\mathcal{L} = \sum_\mu l(f_\mu, y_\mu)$ と学習率 $\eta$ を用いて、
        \begin{align}
            \frac{d\boldsymbol{\theta}}{dt} &= -\eta\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}}
        \end{align}
        と時間発展させる。
        ただし $\boldsymbol{\theta} = \text{Vec}\{\boldsymbol{W}^0, \dots, \boldsymbol{W}^{L-1}, \boldsymbol{w}_L\}$ は学習する全ての重みをベクトル化したものである。
    </p>
    <p>
        さて、ここで考えたいのは出力の学習ダイナミクス $f_\mu(t)$ （正確にいうと、全てのデータを使って重みを学習した時の、一つのデータ$\boldsymbol{x}_\mu$に対する出力$f_\mu$の学習ダイナミクス）である。
        ナイーブに考えれば連鎖率から
        $\frac{\partial f_\mu}{\partial t}=\nabla_{\boldsymbol{\theta}=\boldsymbol{\theta}(t)}f_\mu \cdot \frac{d\boldsymbol\theta(t)}{dt}$
        であり、これと上の式たちを使えば学習ダイナミクスを陽に計算できる。
        というかこの計算を離散化して実行するのが通常の深層学習である。
        しかし、理論的に解析する場合には、この方法は以下に示す理由から実用的でも本質的でもない。
    </p>
    
    <h3>ナイーブな方法の欠点</h3>
    <ul>
      <li>ネットワークの各層の幅 $N$ を無限大にする極限では計算が無限に煩雑になってしまう</li>
      <li>重みはランダムに初期化されるので、それぞれのrealizationごとにダイナミクスを調べても一般的な事実が得られない</li>
    </ul>

    <p>
        これは「理解」に関するよくある議論の具体例だと思う。
        要素の数がきわめて多くなったり相互作用がランダムになったりする場合には、それぞれの要素や相互作用を個別に調べるだけでは「理解」したことにはならない。
        その代わりに巨視的な量やその期待値を計算することで初めて、系の典型的な性質がわかる。
        動的平均場理論はまさにそのための手続きである。
    </p>

    <h3>この記事の目標と構成</h3>
    <p>
        上で説明した通り、いま手元にあるのは、出力の学習ダイナミクスを微視的なパラメータで記述した式
        $\frac{\partial f_\mu}{\partial t}=\nabla_{\boldsymbol{\theta}=\boldsymbol{\theta}(t)}f_\mu \cdot \frac{d\boldsymbol\theta(t)}{dt}$
        である。
        それでは不便なので、パラメータではなく巨視変数であるカーネルで記述したい。
        より正確には、パラメータのダイナミクスを一切参照せずに、<strong>カーネルと出力だけで閉じた時間発展方程式を得たい</strong>。
    </p>
    <p>
        これを実現するために、まず出力のダイナミクスをカーネルで表示する。
        この部分は割と簡単なので直後に説明する。
        難しいのはそのカーネルの時間発展をパラメータを用いずに表す部分だ。
        動的平均場理論の核心はここにあるので、計算の前にその気持ちを紹介する。
        具体的な計算は非常に複雑なため、中間層が1層の場合でウォームアップしてから、一般の深層ネットワークの場合を説明する。
    </p> 

    <h3>出力ダイナミクスをカーネルで表示する</h3>
    <details>
        <summary>$\frac{\partial f_\mu}{\partial t}=\sum_\alpha K_{\mu\alpha}^{NTK} (t,t) \Delta_\alpha (t)$ と表せる（クリックして表示）</summary>
        <p>
            NTKカーネル $K_{\mu\alpha}^{NTK} (t,s)$ とは、ニューラルネットワークの出力の勾配の内積である。
            $$
            K_{\mu\alpha}^{NTK} (t,s) = \gamma^2 \nabla_{\boldsymbol{\theta}(t)} f_\mu(t) \cdot \nabla_{\boldsymbol{\theta}(s)} f_\alpha(s)
            $$
            通常の定義では係数 $\gamma^2$ は付かないが、今の設定では出力 $f$ が $1/\gamma$ 倍されているのでそれを打ち消すために付けている。
            NTKはNeural Tangent Kernelの略なのでNTカーネルと呼ぶのが正しいのかもしれないが、変な感じがするのでこの記事ではNTKカーネルと呼ぶ。
        </p>
        <p>
            ではこのNTKカーネルを使って出力ダイナミクスが書けることを示そう。
            \begin{align}
                \frac{\partial f_\mu}{\partial t} &= \nabla_{\boldsymbol{\theta}(t)}f_\mu(t) \cdot \frac{d\boldsymbol\theta(t)}{dt} \\
                &= -\eta\nabla_{\boldsymbol{\theta}(t)}f_\mu(t) \cdot\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}} \\
                &= \eta\nabla_{\boldsymbol{\theta}(t)}f_\mu(t) \cdot \sum_\alpha \nabla_{\boldsymbol{\theta}(t)} f_\alpha(t) \Delta_\alpha(t) \\
                &= \eta \sum_\alpha \nabla_{\boldsymbol{\theta}(t)}f_\mu(t) \cdot \nabla_{\boldsymbol{\theta}(t)} f_\alpha(t) \Delta_\alpha(t) \\
                &= \frac{\eta}{\gamma^2}\sum_\alpha K_{\mu\alpha}^{NTK} (t,t) \Delta_\alpha(t)
            \end{align}
            ここで $\Delta_\mu(t) = \left.-\frac{\partial\mathcal{L}}{\partial f_\mu}\right|_{f_\mu=f_\mu(t)}$ は誤差関数の微分で、設定に応じて具体的に書き下せる。
            例えばMSE lossの場合は $\Delta_\mu(t) = y_\mu - f_\mu(t)$ である。また、学習が $O_\gamma(1)$ で進むように以降では学習率を $\eta=\gamma^2$ とする。
            以上から題意は示された。
        </p>
        <p>
            この結果を味わってみよう。
            $\Delta_\alpha(t)$ は出力 $f_\alpha(t)$ と教師データ $y_\alpha$ を使って簡単に書き下せるので、$f(t)$ の時間発展を考えるにあたっては何も問題ない。
            逆にNTKカーネル $K_{\mu\alpha}^{NTK} (t,t)$ の方はパラメータに依存してしまっている。
            したがってNTKカーネルの時間発展をパラメータを用いずに巨視変数だけで表すことができれば、出力が閉じた形で表せる。
        </p>
        <p>
            また別の見方をすると、データ $\alpha$ に対する出力の学習がNTKカーネル $K_{\mu\alpha}^{NTK} (t,t)$ を通して他のデータ $\mu$ の学習に影響を与えているともいえる。
        </p>
        <p class="close-btn">（クリックして非表示）</p>
    </details>
    <details>
        <summary>$K_{\mu\alpha}^{NTK} (t,s) = \sum_{l=0}^L \Phi_{\mu\alpha}^{l} (t,s)G_{\mu\alpha}^{l+1} (t,s)$ と表せる（クリックして表示）</summary>
        <p>
            特徴量カーネル $\Phi_{\mu\alpha}^{l} (t,s)$ と勾配カーネル $G_{\mu\alpha}^{l} (t,s)$ をそれぞれ次のように定義する。
            \begin{align}
                \Phi_{\mu\alpha}^{l} (t,s) &:= \frac{1}{N} \phi(\boldsymbol{h}_\mu^l(t)) \cdot \phi(\boldsymbol{h}_\alpha^l(s)) \\
                G_{\mu\alpha}^{l} (t,s) &:= \frac{1}{N} \boldsymbol{g}_\mu^l(t) \cdot \boldsymbol{g}_\alpha^l(s)
            \end{align}
            ただし勾配ベクトル $\boldsymbol{g}_\mu^l(t)$ の定義は次の通り。
            $$
                \boldsymbol{g}_\mu^l(t) := \gamma\sqrt{N} \frac{\partial f_\mu(t)}{\partial \boldsymbol{h}^l_\mu(t)}
            $$
        </p>
        <p>
            直感的には、特徴量カーネル $\Phi_{\mu\alpha}^{l} (t,s)$ はデータ $\mu,\alpha$ それぞれに対する第 $l$ 層の活性化ベクトル $\phi \left( \boldsymbol{h}^l_\mu(t) \right),\phi \left( \boldsymbol{h}^l_\alpha(s) \right)$ の内積、
            勾配ベクトル $\boldsymbol{g}_\mu^l(t)$ は第 $l$ 層の特徴量を変化させた時の出力の変化を表すベクトル、
            勾配カーネル $G_{\mu\alpha}^{l} (t,s)$ はデータ $\mu,\alpha$ それぞれに対する第 $l$ 層の勾配ベクトルの内積である。
        </p>
        <p>
            NTKカーネルの定義を変形して題意の式を得よう。
            \begin{align}
                K_{\mu\alpha}^{NTK} (t,s) &= \gamma^2 \nabla_{\boldsymbol{\theta}(t)} f_\mu(t) \cdot \nabla_{\boldsymbol{\theta}(s)} f_\alpha(s) \\
                &= \gamma^2 \sum_{l=0}^L \sum_{i,j=1}^N \nabla_{W_{ij}^l(t)} f_\mu(t) \cdot \nabla_{W_{ij}^l(s)} f_\alpha(s) \\
            \end{align}
        </p>
        <p>
            ここで
            \begin{align}
                \nabla_{W_{ij}^l(t)} f_\mu(t) &= \frac{\partial f_\mu(t)}{\partial \boldsymbol{h}_\mu^{l+1}(t)} \cdot \frac{\partial \boldsymbol{h}_\mu^{l+1}(t)}{\partial W_{ij}^l(t)} \\
                &= \left(\frac{\partial f_\mu(t)}{\partial \boldsymbol{h}_\mu^{l+1}(t)}\right)_i  \left(\frac{1}{\sqrt{N}} \phi(\boldsymbol{h}_\mu^l(t))\right)_j \\
                &= \left(\frac{1}{\gamma\sqrt{N}}\boldsymbol{g}_\mu^{l+1}(t)\right)_i  \left(\frac{1}{\sqrt{N}} \phi(\boldsymbol{h}_\mu^l(t))\right)_j \\
            \end{align}
            を代入すると、
        </p>
        <p>
            \begin{align}
                K_{\mu\alpha}^{NTK} (t,s) &= \gamma^2 \sum_{l=0}^L \left(\frac{1}{\gamma^2N}\sum_{i=1}^N \left(\boldsymbol{g}_\mu^{l+1}(t)\right)_i
                \left(\boldsymbol{g}_\alpha^{l+1}(s)\right)_i \right)
                \left(\frac{1}{N}\sum_{j=1}^N \left( \phi(\boldsymbol{h}_\mu^l(t))\right)_j \left( \phi(\boldsymbol{h}_\alpha^l(s))\right)_j \right) \\
                &= \sum_{l=0}^L \frac{1}{N}\boldsymbol{g}_\mu^{l+1}(t)\cdot\boldsymbol{g}_\alpha^{l+1}(s) \frac{1}{N}\phi(\boldsymbol{h}_\mu^l(t))\cdot\phi(\boldsymbol{h}_\alpha^l(s)) \\
                &= \sum_{l=0}^L \Phi_{\mu\alpha}^{l} (t,s)G_{\mu\alpha}^{l+1} (t,s) \\
            \end{align}
            以上から題意は示された。
            ただし端の層には注意が必要。
        </p>
        <details>
            <summary>入力層 $l=0$ について（クリックして表示）</summary>
            <p>
                $\nabla_{W_{ij}^0(t)} f_\mu(t) = \left(\frac{\partial f_\mu(t)}{\partial \boldsymbol{h}_\mu^{1}(t)}\right)_i \cdot \left(\frac{1}{\sqrt{D}} \boldsymbol{x}_\mu\right)_j \\$なので、
                $$
                    \gamma^2 \nabla_{\boldsymbol{W}^0(t)} f_\mu(t) \cdot \nabla_{\boldsymbol{W}^0(s)} f_\alpha(s)  = G_{\mu\alpha}^{1} (t,s) \frac{1}{D} \boldsymbol{x}_\mu \cdot  \boldsymbol{x}_\alpha
                $$
                よって一貫性のために $\Phi_{\mu\alpha}^{0} (t,s) = \frac{1}{D} \boldsymbol{x}_\mu \cdot \boldsymbol{x}_\alpha = K_{\mu\alpha}^{x}$ (入力のグラム行列)とする。
            </p>
            <p class="close-btn">（クリックして非表示）</p>
        </details>
        <details>
            <summary>最終層 $l=L$ について（クリックして表示）</summary>
            <p>
                $\nabla_{\boldsymbol{w}^L(t)} f_\mu(t) = \frac{1}{\gamma\sqrt{N}} \phi(\boldsymbol{h}_\mu^L(t))$なので、
                $$
                    \gamma^2 \nabla_{\boldsymbol{w}^L(t)} f_\mu(t) \cdot \nabla_{\boldsymbol{w}^L(s)} f_\alpha(s)  = \Phi_{\mu\alpha}^{L} (t,s)
                $$
                よって一貫性のために $G_{\mu\alpha}^{L+1} (t,s) = \boldsymbol{1}$ とする。
            </p>
            <p class="close-btn">（クリックして非表示）</p>
        </details>
        <p>
            この結果を味わってみよう。
            このDNNには層が $L$ 枚あって各層に $N^2$ 個のパラメータがあるので、パラメータの総数は大体 $N^2L$ である。
            NTKカーネルは全てのパラメータに関する勾配の情報なので、愚直にやると計算量は $O(N^2L)$である。
            層の幅 $N$ を無限大にする極限を考えたいのにとんでもない。
        </p>
        <p>
            しかし、特徴量カーネルと勾配カーネルは両方とも $O(N)$ で計算できるので、導出した関係式を使えばNTKカーネルの計算が $O(NL)$ でできる。
            ここからさらに $O(L)$ にするために動的平均場理論を使う。
        </p>
        <p class="close-btn">（クリックして非表示）</p>

    </details>
    <p>
        以上から、特徴量カーネル $\Phi$ と勾配カーネル $G$ を使って出力の時間発展が次のように表せる。
        $$
            \frac{\partial f_\mu}{\partial t} = \sum_\alpha \sum_{l=0}^L \Phi_{\mu\alpha}^{l} (t,t)G_{\mu\alpha}^{l+1} (t,t) \Delta_\alpha(t)
        $$
        次の問題はカーネル $\Phi, G$ の時間発展を求めることだ。
        各カーネルはそれぞれ特徴量ベクトルと勾配ベクトルで定義されているので、各ベクトルの時間発展を求めれば良い。
    </p>

    <h3>特徴量ベクトルと勾配ベクトルの時間発展</h3>
    <details>
        <summary>
            $\boldsymbol{h}^{l+1}_\mu(t) 
            = \frac{1}{\sqrt{N}}\boldsymbol{W}^{l}(0)\phi(\boldsymbol{h}^l_\mu(t))
            + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \boldsymbol{g}_\nu^{l+1}(s)\Phi_{\nu\mu}^{l} (s,t)$（クリックして表示）
        </summary>
        <p>
            これは特徴量ベクトル $\boldsymbol{h}^{l+1}_\mu$ の時間発展を、隣の層の特徴量ベクトル $\boldsymbol{h}^l_\nu$、同じ層の勾配ベクトル $\boldsymbol{g}_\nu^{l+1}$、隣の層の特徴量カーネル $\Phi_{\nu\mu}^{l}$ を使って表したものである。
            これを導出しよう。
            定義から $\boldsymbol{h}^{l+1}_\mu(t) = \frac{1}{\sqrt{N}}\boldsymbol{W}^{l}(t)\phi(\boldsymbol{h}^l_\mu(t))$ なので、パラメータ $\boldsymbol{W}^l(t)$ の時間発展を求めれば良い。
        </p>
        <p>
            パラメータが満たす微分方程式を変形すると、
            \begin{align}
                \frac{dW_{ij}^{l}(t)}{dt} &= -\gamma^2\nabla_{W_{ij}^{l}(t)} \mathcal{L}(f(t)) \\
                &= \gamma^2\sum_\mu\Delta_\mu(t) \frac{\partial f_\mu}{\partial \boldsymbol{h}_\mu^{l+1}(t)} \cdot \frac{\partial \boldsymbol{h}_\mu^{l+1}(t)}{\partial W_{ij}^{l}(t)} \\
                &= \gamma^2\sum_\mu\Delta_\mu(t) \left(\frac{\boldsymbol{g}_\mu^{l+1}(t)}{\gamma\sqrt{N}}\right)_i \left(\frac{\phi(\boldsymbol{h}_\mu^l(t))}{\sqrt{N}}\right)_j \\
                &= \frac{\gamma}{N}\sum_\mu\Delta_\mu(t) \left(\boldsymbol{g}_\mu^{l+1}(t)\right)_i \left(\phi(\boldsymbol{h}_\mu^l(t))\right)_j \\
                \therefore \boldsymbol{W}^{l}(t) &= \boldsymbol{W}^{l}(0) + \frac{\gamma}{N}\int_0^t ds \sum_\mu\Delta_\mu(s) \boldsymbol{g}_\mu^{l+1}(s)\phi(\boldsymbol{h}_\mu^l(s))^\top \\
            \end{align}
        </p>
        <p>
            これを $\boldsymbol{h}^{l+1}_\mu(t) = \frac{1}{\sqrt{N}}\boldsymbol{W}^{l}(t)\phi(\boldsymbol{h}^l_\mu(t))$ に代入して目的の式を得る。
            流れをまとめると、第 $l$ 層と第 $l+1$ 層の特徴量ベクトルの関係式にパラメータの時間発展を代入して、特徴量ベクトルの時間発展を求めている。
        </p>
        <p class="close-btn">（クリックして非表示）</p>
    </details>
    <details>
        <summary>
            $
            \left\{
            \begin{aligned}
                &\boldsymbol{g}^{l}_\mu(t) = \dot{\phi}(\boldsymbol{h}^l_\mu(t)) \odot \boldsymbol{z}^{l}_\mu(t) \\
                &\boldsymbol{z}^{l}_\mu(t) = \frac{1}{\sqrt{N}}\boldsymbol{W}^{l\top}(0)\boldsymbol{g}^{l+1}_\mu(t)
                + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \phi(\boldsymbol{h}_\nu^{l}(s))G_{\nu\mu}^{l+1} (s,t)
            \end{aligned}
            \right.
            $（クリックして表示）
        </summary>
        <p>
            前項の流れを踏襲したいので、まず勾配ベクトルの漸化式を求める。
            特徴量ベクトルの場合は定義から明らかに順方向の漸化式が得られた。
            勾配ベクトルの場合は微分の連鎖率から逆方向の漸化式が得られる。
            \begin{align}
                \left(\boldsymbol{g}^{l}_\mu(t)\right)_i &= \gamma\sqrt{N}\left(\frac{\partial f_\mu}{\partial \boldsymbol{h}_\mu^{l}(t)}\right)_i \\
                &= \gamma\sqrt{N}\frac{\partial f_\mu}{\partial \boldsymbol{h}_\mu^{l+1}(t)} \cdot \frac{\partial \boldsymbol{h}_\mu^{l+1}(t)}{\left(\partial \boldsymbol{h}_\mu^{l}(t)\right)_i} \\
                &= \sum_j \left(\boldsymbol{g}_\mu^{l+1}(t)\right)_j \frac{\partial \left(\boldsymbol{h}_\mu^{l+1}(t)\right)_j}{\partial \left(\boldsymbol{h}_\mu^{l}(t)\right)_i} \\
                &= \sum_j \left(\boldsymbol{g}_\mu^{l+1}(t)\right)_j \frac{1}{\sqrt{N}}W^l_{ji}(t)\dot{\phi}(\boldsymbol{h}_\mu^{l}(t))_i \\
                &= \frac{1}{\sqrt{N}}\dot{\phi}(\boldsymbol{h}_\mu^{l}(t))_i\left(\boldsymbol{W}^{l\top}(t)\boldsymbol{g}_\mu^{l+1}(t)\right)_i \\
                \therefore \boldsymbol{g}^{l}_\mu(t) &= \dot{\phi}(\boldsymbol{h}_\mu^{l}(t)) \odot \boldsymbol{z}^{l}_\mu(t)
            \end{align}
            ただし $\boldsymbol{z}^{l}_\mu(t) = \frac{1}{\sqrt{N}}\boldsymbol{W}^{l\top}(t)\boldsymbol{g}^{l+1}_\mu(t)$ 。
            これは誤差逆伝播の計算と全く同じ。
            こうして求めた漸化式に前項で求めた $\boldsymbol{W}^l(t)$ を代入して目的の式を得る。
        </p>
        <p class="close-btn">（クリックして非表示）</p>
    </details>

    <figure class="article__image-container">
        <img src="../images/dmft1/dependency.jpeg">
        <figcaption>図1: 変数の依存関係</figcaption>
    </figure>
    <p>
        注目するべきは、学習によって時間変化するパラメータ $\boldsymbol{W}^{l}(t)$ はもはや登場せず、パラメータの寄与は初期値 $\boldsymbol{W}^{l}(0)$ のみに現れている点だ。
        別の言い方をすれば、各ベクトルの初期化だけにランダムネスが現れ、時間発展の方は出力とベクトルとカーネルで閉じた形になっている。
        変数の依存関係を図１にまとめた。
        $\boldsymbol{W}^{l}(t)$ の分布は非常に複雑な一方で初期値 $\boldsymbol{W}^{l}(0)$ の分布はこちらが指定できるため、これは縁起の良い形だ。
    </p>


    <h3>動的平均場理論</h3>
    <p>
        前置きが長くなったが、ここからが本題。
        この記事の目標は幅無限大の極限で出力の学習ダイナミクスを理論解析することだった。
        素朴に考えるとパラメータの時間発展が主役になってしまうが、ここまでの計算によってパラメータの初期値とベクトルの時間発展で記述できるようになった。
        残る問題は①パラメータの初期化にランダムネスがあることと、②ベクトルの長さが無限大になってしまうこと。
        動的平均場理論を用いて、①ガウス積分で期待値を取る方法と②モーメント生成関数を変形してベクトルではなくスカラーで記述する方法とでそれぞれ解決する。
    </p>
    <p>
        動的平均場理論(Dynamical Mean Field Theory, DMFT)とは、1.クエンチした確率変数が 2.大量に集まった系での、 3.巨視変数の 4.ダイナミクスを求める方法である。
    </p>
    <ol>
        <li>
            確率変数がクエンチしているとは、注目する時間発展の間に変化しないという意味である。
            例えばリカレントネットワークの結合をランダムに定めて、その結合のもとでニューロンの状態を時間発展させる系や、フィードフォワードネットワークの初期重みをランダムに定めて出力を学習する系(今の系はこれ)が考えられる。
            後者の場合は重みが時間発展するが、確率性があるのは重みの初期化だけで、それが決まれば残りは決定論的に発展するという点がポイントだと思う。
        </li>
        <li>
            クエンチした独立な確率変数が大量に集まる時、自己平均性が期待できる。
            この辺あやふやなので後で書き換えるかも。
        </li>
        <li>
            巨視的な変数は、Ising系なら磁化、リカレントネットワークなら平均活動度、フィードフォワードネットワークなら上で定義した特徴量カーネル $\Phi_{\mu\alpha}$ と勾配カーネル $G_{\mu\alpha}$ などのように、微視的な変数を平均して求める変数のこと。
            オーダーパラメータともいう。
        </li>
        <li>
            例えばIsingモデルの平均場近似とは違って、ダイナミクスを記述することから動的という名前がついている（と僕は思っている）。
            したがって求めるべき対象は、巨視変数のダイナミクスを閉じた形で定める時間発展方程式や、相関関数のセルフコンシステント方程式である。
        </li>
    </ol>

    <h3>ウォームアップ：L=1の場合</h3>
    <figure class="article__image-container">
        <img src="../images/dmft1/onelayer.jpeg">
        <figcaption>図2: L=1の場合</figcaption>
    </figure>
    <p>
        この節では中間層が1層の場合を考える(図2)。
        出力のダイナミクスを記述するにあたって必要なのは $\Phi_{\mu\alpha}^0, \Phi_{\mu\alpha}^1, G_{\mu\alpha}^1, G_{\mu\alpha}^2$ だ。
        このうち端の二つは定義から $\Phi_{\mu\alpha}^0 = K_{\mu\alpha}^x, G_{\mu\alpha}^2 = \boldsymbol{1}$ となるので、中の二つが問題となる。
    </p>
    <p>
        定義から $\Phi_{\mu\alpha}^1 = \frac{1}{N} \phi(\boldsymbol{h}_\mu) \cdot \phi(\boldsymbol{h}_\alpha), 
        G_{\mu\alpha}^1 = \frac{1}{N} \boldsymbol{g}_\mu \cdot \boldsymbol{g}_\alpha$ なので、
        $\boldsymbol{h}_\mu(t), \boldsymbol{g}_\mu(t)$ を計算すれば良い(ここで層の添字 $l$ は省略している)。
        上でやった計算を境界条件に気をつけて再現すれば、
        \begin{align}
            \boldsymbol{h}_\mu(t) &= \frac{1}{\sqrt{D}} \boldsymbol{W}^{0}(t)\boldsymbol{x}_\mu \\
            &= \frac{1}{\sqrt{D}}\left(\boldsymbol{W}^{0}(0) + \frac{\gamma}{\sqrt{ND}}\int_0^t ds \sum_\nu\Delta_\nu(s) \boldsymbol{g}_\nu(s)\boldsymbol{x}_\nu^\top\right)\boldsymbol{x}_\mu \\
            &= \boldsymbol{\chi}_\mu + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \boldsymbol{g}_\nu(s)K_{\mu\nu}^x \\
            \boldsymbol{g}_\mu(t) &= \boldsymbol{z}(t) \odot \dot\phi(\boldsymbol{h}_\mu(t)) \\
            \boldsymbol{z}(t) &= \boldsymbol{w}^{1}(t)\\
            &= \boldsymbol{\xi} + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \phi(\boldsymbol{h}_\nu(s))\\
        \end{align}
        となる。
        ただし第一項をそれぞれ $\boldsymbol{\chi}_\mu := \frac{1}{\sqrt{D}}\boldsymbol{W}^{0}(0)\boldsymbol{x}_\mu, \boldsymbol{\xi} := \boldsymbol{w}^{1}(0)$ とおいた。
        どちらも $t$ に依存しない定数であることに注意しよう。
        ネタバレしておくと一般の $L$ ではこれが時間変化するようになって大変になる。
    </p>
    <p>
        パラメータ初期化のランダムネスは $\boldsymbol{\chi}_\mu$ と $\boldsymbol{\xi}$ にのみ現れていて、それがベクトル $\boldsymbol{h}_\mu(t), \boldsymbol{g}_\mu(t)$ に影響し、カーネル $\Phi_{\mu\alpha}^1, G_{\mu\alpha}^1$ に影響している。
        この微視的なランダムネスをまともに扱うと $O(N)$ になってしまうが、知りたいのはカーネル $\Phi_{\mu\alpha}^1, G_{\mu\alpha}^1$ であることを念頭に置いて、これを回避する。
        抽象的には、パラメータ初期値という微視変数からカーネルという巨視変数に変数変換をすることで測度の集中が起きるということ。
        具体的には、 $\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)$ のモーメント生成関数を変形することで、カーネル $\Phi_{\mu\alpha}^1, G_{\mu\alpha}^1$ の $O(1)$ の計算方法を入手するということ。
    </p>

    <details>
        <summary>モーメント生成関数 $Z(j)$ の導入（クリックして表示）</summary>
        <p>
            以降の計算で主役になるのはモーメント生成関数 (モーメント母関数)だ。
            一般に、確率変数 $x$ のモーメント生成関数 $Z_x(j)$ は次のように定義される。
            \begin{align}
                Z_x(j) &= \langle \exp(jx) \rangle \\
                &= \int P(x) \exp(jx) dx
            \end{align}
            ただし $P(x)$ は $x$ の確率分布である。
            なぜモーメント生成関数と呼ぶかというと $j$ で $k$ 回微分したときに $x$ の $k$ 次モーメントが現れるからだが、その性質は今回の記事では使わない。
            重要なのは、$Z_x(j)$ が $P(x)$ の完全な情報を持っているという点だ。
            以降の計算では $W(0)$ の確率性に注目してモーメント生成関数を定義した後に、それを変形し、新しく現れたモーメント生成関数を見て平均場の確率分布が得られたとみなす。
        </p>
        <details>
            <summary>モーメント生成関数と分配関数の関係（クリックして表示）</summary>
            <p>
                モーメント生成関数を $Z$ で表すのはある意味で分配関数と同一視できるからだ。
                それを説明する。
                統計力学では分配関数 $Z$ は次のように定義される。
                \begin{align}
                    Z(\beta) &= \int \exp(-\beta E(x)) dx \\
                    &= \int \rho(E)\exp(-\beta E) dE
                \end{align}
                ただし $\beta$ は逆温度であり、$\rho(E)$ はエネルギー $E$ の状態密度である。
            </p>
            <p>
                モーメント生成関数の定義と見比べると、
                \begin{align}
                    x &\leftrightarrow E \\
                    P(x) &\leftrightarrow \rho(E) \\
                    j &\leftrightarrow -\beta \\
                \end{align}
                という対応があることがわかる。
                ただしモーメント生成関数の方は高次元の $x$ と$j$ を考えうるのに対して、分配関数はそうではないと思うので、完璧な対応とはいえないかもしれない。
            </p>
            <p class="close-btn">（クリックして非表示）</p>
        </details>
        <p class="close-btn">（クリックして非表示）</p>
    </details>

    <p>
        $\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)$ に関するモーメント生成関数を定義する。
        $$
            Z[\{\boldsymbol{j}_\mu\}_{\mu \in [P]}, \boldsymbol{v}] := \left\langle \exp\left(\sum_\mu \boldsymbol{j}_\mu \cdot \boldsymbol{\chi}_\mu + \boldsymbol{v}\cdot\boldsymbol{\xi}\right)\right\rangle_{\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)}
        $$
    </p>
    <details>
        <summary>
            恒等式を挟んで変形すると次のようになる。（クリックして表示）
        </summary>
        <p>
            いま $\boldsymbol{\chi}_\mu = \frac{1}{\sqrt{D}}\boldsymbol{W}^0(0)\boldsymbol{x}_\mu$ という関係式がある。
            これを上の式に代入するのではなく、$\delta$関数として表現し、さらにそれをフーリエ変換した恒等式を用いる。
            すなわち
            \begin{align}
                1 &= \int d\boldsymbol{\chi}_\mu \delta\left(\boldsymbol{\chi}_\mu - \frac{1}{\sqrt{D}}\boldsymbol{W}^0(0)\boldsymbol{x}_\mu\right)\\
                &= \int \frac{d \boldsymbol{\chi}_\mu d \boldsymbol{\hat\chi}_\mu}{(2\pi)^N}\exp\left(i\boldsymbol{\hat\chi}_\mu\left(\boldsymbol{\chi}_\mu - \frac{1}{\sqrt{D}}\boldsymbol{W}^0(0)\boldsymbol{x}_\mu\right)\right)
            \end{align}
            を全ての $\mu$ について両辺に掛ける。
        </p>
        <p>
            $\xi$ についても同様に、
            $$
            1=\int \frac{d \boldsymbol{\xi} d \boldsymbol{\hat\xi}}{(2\pi)^N} \exp\left(i\boldsymbol{\hat\xi} \left(\boldsymbol{\xi} - \boldsymbol{w}^1(0)\right)\right)
            $$
            を両辺に掛ける。
        </p>
        <p>
            こうすることで、元は $\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)$ の関数だった $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ が独立な確率変数として扱えるようになり、
            $\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)$ の寄与は陽に現れるようになった。
            その結果パラメータ初期値に関する期待値が取りやすくなった（本当はL=1の場合は元から取りやすかったが、一般のLでは真に取りやすくなる）。
            著者によるとこういう形式をMSRDJ formalismというらしい。
            MSRDJ経路積分という経路積分表示で似たような変形をするからだと思う。
        </p>
        <p class="close-btn">(クリックして非表示)</p>
    </details>
    <p>
        \begin{align}
            Z[\{\boldsymbol{j}_\mu\}_{\mu \in [P]}, \boldsymbol{v}]
            = \int \left(\prod_\mu \frac{d \boldsymbol{\chi}_\mu d \boldsymbol{\hat\chi}_\mu}{(2\pi)^N}\right)
            \frac{d \boldsymbol{\xi} d \boldsymbol{\hat\xi}}{(2\pi)^N}
            \left\langle\exp\left(
                \sum_\mu i\boldsymbol{\hat\chi}_\mu \left(\boldsymbol{\chi}_\mu - \frac{1}{\sqrt{D}}\boldsymbol{W}^0(0)\boldsymbol{x}_\mu\right) \right. \right.&\\
                \left.\left. + i\boldsymbol{\hat\xi} \left(\boldsymbol{\xi} - \boldsymbol{w}^1(0)\right)
                + \sum_\mu \boldsymbol{j}_\mu \cdot \boldsymbol{\chi}_\mu + \boldsymbol{v}\cdot\boldsymbol{\xi}
            \right)\right\rangle_{\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)}&
        \end{align}
    </p>

    <details>
        <summary>
            $\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)$ についてガウス積分すると $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ のモーメント生成関数が得られる。（クリックして表示）
        </summary>
        <p>
            $x\sim \mathcal{N}(0, \sigma^2)$ のガウス変数について、次の公式が成り立つ。
            \begin{align}
                \langle \exp(jx) \rangle_x &= \int dx \exp\left(-\frac{x^2}{2\sigma^2} + jx\right)\\
                &= \int dx \exp\left(-\frac{1}{2\sigma^2}(x - j\sigma^2)^2 + \frac{j^2\sigma^2}{2}\right)\\
                &= \exp\left(\frac{j^2\sigma^2}{2}\right)
            \end{align}
            これを念頭に置いて高次元の場合も積分しよう。
        </p>
        <p>
            \begin{align}
                \left\langle \exp\left(-\sum_\mu \frac{i\boldsymbol{\hat\chi}_\mu \boldsymbol{W}^0(0) \boldsymbol{x}_\mu}{\sqrt{D}}\right) \right\rangle_{\boldsymbol{W}^0(0)}
                &= \prod_{ij}\left\langle\exp\left(-\sum_\mu\frac{i(\hat\chi_\mu)_i W^0_{ij}(0) (x_\mu)_j}{\sqrt{D}}\right)\right\rangle_{W^0_{ij}(0)} \\
                &= \prod_{ij}\exp\left(-\frac{1}{2D}\sum_{\mu\alpha} (\hat\chi_\mu)_i(\hat\chi_\alpha)_i(x_\mu)_j(x_\alpha)_j\right) \\
                &= \exp\left(-\frac{1}{2D}\sum_{\mu\alpha} \boldsymbol{\hat\chi}_\mu \cdot \boldsymbol{\hat\chi}_\alpha \boldsymbol{x}_\mu \cdot \boldsymbol{x}_\alpha\right) \\
                &= \exp\left(-\frac{1}{2}\sum_{\mu\alpha} \boldsymbol{\hat\chi}_\mu \cdot \boldsymbol{\hat\chi}_\alpha K^x_{\mu\alpha}\right) \\

                \left\langle \exp\left(-i\boldsymbol{\hat\xi} \boldsymbol{w}^1(0)\right) \right\rangle_{\boldsymbol{w}^1(0)}
                &= \prod_{i}\left\langle\exp\left(-i\hat\xi_i w^1_i(0)\right)\right\rangle_{w^1_i(0)} \\
                &= \prod_{i}\exp\left(-\frac{1}{2}\hat\xi_i^2\right) \\
                &= \exp\left(-\frac{1}{2}|\boldsymbol{\hat\xi}|^2\right) 
            \end{align}
            上の二式を代入すれば $\boldsymbol{W}^0(0), \boldsymbol{w}^1(0)$ のガウス積分が完了する。
            こうしてパラメータの初期値を変数変換してベクトル $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ の確率性に注目したモーメント生成関数が手に入った。
        </p>
        <p class="close-btn">(クリックして非表示)</p>
    </details>
    <p>
        \begin{align}
            Z[\{\boldsymbol{j}_\mu\}_{\mu \in [P]}, \boldsymbol{v}]
            = \int \left(\prod_\mu \frac{d \boldsymbol{\chi}_\mu d \boldsymbol{\hat\chi}_\mu}{(2\pi)^N}\right) \frac{d \boldsymbol{\xi} d \boldsymbol{\hat\xi}}{(2\pi)^N}
            \exp\left(
                \sum_\mu i\boldsymbol{\hat\chi}_\mu \cdot \boldsymbol{\chi}_\mu
                - \frac{1}{2}\sum_{\mu\alpha} \boldsymbol{\hat\chi}_\mu \cdot \boldsymbol{\hat\chi}_\alpha K_{\mu\alpha}^x \right. &\\
                \left. + i\boldsymbol{\hat\xi} \cdot \boldsymbol{\xi}
                - \frac{1}{2}|\boldsymbol{\hat\xi}|^2
                + \sum_\mu \boldsymbol{j}_\mu \cdot \boldsymbol{\chi}_\mu
                + \boldsymbol{v}\cdot\boldsymbol{\xi}
            \right) &
        \end{align}
    </p>
    <p>
        このモーメント生成関数をよく見ると、独立な $N$ 個の1体問題の積に分解できることがわかる。
        すなわち、
        $$
        \left\{
            \begin{align}
                Z[\{\boldsymbol{j}_\mu\}_{\mu \in [P]}, \boldsymbol{v}]
                &= \prod_{i=1}^N Z_1[\{j^\mu_i\}_{\mu \in [P]}, v_i] \\

                Z_1[\{j^\mu\}_{\mu \in [P]}, v]
                &= \int \left(\prod_\mu \frac{d\chi_\mu d \hat\chi_\mu}{2\pi}\right)\frac{d\xi d\hat\xi}{2\pi}
                \exp\left( S_1[\{\chi_\mu\}, \xi] \right) \\
                S_1[\{\chi_\mu\}, \xi] &=
                    \sum_\mu i\hat\chi_\mu\chi_\mu
                    - \frac{1}{2}\sum_{\mu\alpha} \hat\chi_\mu \hat\chi_\alpha K_{\mu\alpha}^x
                    + i\hat\xi \xi
                    - \frac{1}{2}\hat\xi^2
                    + \sum_\mu j^\mu \chi_\mu
                    + v \xi
            \end{align}
        \right.
        $$
        と書ける。$\chi_\mu, \xi, j^\mu, v$ はいずれもスカラーである。
    </p>
    <p>
        1体のモーメント生成関数 $Z_1$ は $i$ によらない点に注目しよう。
        N体のモーメント生成関数 $Z$ がidenticalな $Z_1$ の積で書けるということは、ベクトル $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ の各成分が独立同分布に従う確率変数であることを意味する。
    </p>
    <p>
        しかし、これは $\boldsymbol{h}_\mu(t), \boldsymbol{g}_\mu(t)$ の各成分が独立であることを意味<strong>しない</strong>。
        表式を再掲すると、
        $$
        \left\{
        \begin{align}
            \boldsymbol{h}_\mu(t) &= \boldsymbol{\chi}_\mu + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \boldsymbol{g}_\nu(s)K_{\mu\nu}^x \\
            \boldsymbol{g}_\mu(t) &= \boldsymbol{z}(t) \odot \dot\phi(\boldsymbol{h}_\mu(t)) \\
            \boldsymbol{z}(t) &= \boldsymbol{\xi} + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \phi(\boldsymbol{h}_\nu(s))\\
        \end{align}
        \right.
        $$
        であり、出力 $f_\mu(t)$ によって定まる $\Delta_\mu(t)$ を通して各成分が相互作用している。
    </p>
    <p>
        これを解決するために、<strong>出力を決定するカーネルを主役にしよう</strong>。
        巨視変数であるカーネルが決定論的になることを示して、ベクトル $\boldsymbol{h}_\mu(t), \boldsymbol{g}_\mu(t)$ の $N$ 個の成分が相互作用するのではなく、各成分が独立にカーネルと相互作用するという描像に帰着する。
        より具体的には、モーメント生成関数をベクトルからカーネルに変数変換して、カーネルの分布について鞍点法が使えることを確認する。
        そして鞍点方程式がカーネルの $O(1)$ の計算方法を与えることを示す。
    </p>
    <p>
        したがって、巨視変数であるカーネルの期待値は次のように求められる。
        \begin{align}
            \Phi^1_{\mu\alpha}(t,s) &= \left\langle\frac{1}{N}\phi(\boldsymbol{h}_\mu(t)) \cdot \phi(\boldsymbol{h}_\alpha(s))\right\rangle_{\{\boldsymbol{\chi}_\mu\}, \boldsymbol{\xi}} \\
            &= \left\langle\phi(h_\mu(t)) \phi(h_\alpha(s))\right\rangle_{\{\chi_\mu\}, \xi} \\
            G^1_{\mu\alpha}(t,s) &= \left\langle\frac{1}{N}\boldsymbol{g}_\mu(t) \cdot \boldsymbol{h}_\alpha(s)\right\rangle_{\{\boldsymbol{\chi}_\mu\}, \boldsymbol{\xi}} \\
            &= \left\langle g_\mu(t) g_\alpha(s)\right\rangle_{\{\chi_\mu\}, \xi}
        \end{align}
    </p>
    <details>
        <summary>
            ここで $\langle\rangle_{\{\chi_\mu\}, \xi}$ とは、$\boldsymbol{\chi} \sim \mathcal{N}(0,\boldsymbol{K}^x), \xi \sim \mathcal{N}(0,1)$ というガウシアンでの期待値である。（クリックして表示）
        </summary>
        <p>
            その理由を説明する。
            一体問題のモーメント生成関数は次のようにさらに分解できる。
            $$
            \left\{
            \begin{align}
                Z_1[\{j^\mu\}_{\mu \in [P]}, v] &= Z_1^\chi[\boldsymbol{j}] * Z_1^\xi[v] \\
                Z_1^\chi[\boldsymbol{j}] &= \int \frac{d\boldsymbol{\chi}d \boldsymbol{\hat\chi}}{(2\pi)^P}
                \exp\left(
                    i\boldsymbol{\hat\chi} \cdot \boldsymbol{\chi}
                    - \frac{1}{2} \boldsymbol{\hat\chi}^\top \boldsymbol{K}^x \boldsymbol{\hat\chi}
                    + \boldsymbol{j} \cdot \boldsymbol{\chi}
                \right) \\
                Z_1^\xi[v] &= \int\frac{d\xi d\hat\xi}{2\pi}\exp\left( i\hat\xi \xi - \frac{1}{2}\hat\xi^2 + v \xi \right)
            \end{align}
            \right.
            $$
            ベクトル $\boldsymbol{j}$ や $\boldsymbol{\chi}$ は長さ $N$ ではなく長さ $P$ であることに注意。
            すなわち素子 $i$ ではなくデータ $\mu$ のインデックスを持つ確率変数である。
            同様に $\boldsymbol{K}^x$ は $K_{\mu\alpha}^x$ を $(\mu, \alpha)$ 成分に持つ $P\times P$ 行列である。
        </p>
        <p>
            これをよく見ると、$Z_1^\chi[\boldsymbol{j}]$ は $\boldsymbol{\chi} \sim \mathcal{N}(0,\boldsymbol{K}^x)$ の時のモーメント生成関数で、$Z_1^\xi[v]$ は $\xi \sim \mathcal{N}(0,1)$ の時のモーメント生成関数であることに気づく。
            実際に、$\delta$ 関数のフーリエ変換表示
            $$
                \delta(\boldsymbol{\hat\chi}-i\boldsymbol{j}) = \int \frac{d\boldsymbol{\chi}}{(2\pi)^P} \exp(\boldsymbol{\chi}\cdot(i\boldsymbol{\hat\chi}+\boldsymbol{j}))
            $$
            を用いて、
            \begin{align}
                Z_1^\chi[\boldsymbol{j}] &= \int d\boldsymbol{\hat\chi} \delta(\boldsymbol{\hat\chi}-i\boldsymbol{j})
                \exp\left(- \frac{1}{2} \boldsymbol{\hat\chi}^\top \boldsymbol{K}^x \boldsymbol{\hat\chi}\right) \\
                &= \exp\left(\frac{1}{2} \boldsymbol{j}^\top \boldsymbol{K}^x \boldsymbol{j}\right)
            \end{align}
            と変形でき、これは $\boldsymbol{\chi} \sim \mathcal{N}(0,\boldsymbol{K}^x)$ の時のモーメント生成関数である。
            $\xi$ についても同様。
        </p>
        <p>
            振り返ってみるとモーメント生成関数のガウス積分をした後にガウス積分の逆をしているので、回り道をしたように見える。
            実際十分な洞察力があればガウス積分をする前の形式から独立な1体問題に分解できることを見抜けると思うが、わかりやすさのためにここではこのように進めた。
        </p>
        <p class="close-btn">(クリックして非表示)</p>
    </details>
    <p>
        以上より、出力ダイナミクスを $O(1)$ の計算量で記述することができた。
        $$
        \left\{
        \begin{align}
            \boldsymbol{\chi} &\sim \mathcal{N}(0,\boldsymbol{K}^x) \\
            \xi &\sim \mathcal{N}(0,1) \\
            h_\mu(t) &= \chi_\mu + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) g_\nu(s)K_{\mu\nu}^x \\
            g_\mu(t) &= z(t) \dot\phi(h_\mu(t)) \\
            z(t) &= \xi + \frac{\gamma}{\sqrt{N}}\int_0^t ds \sum_\nu\Delta_\nu(s) \phi(h_\nu(s))\\
            \Phi^1_{\mu\alpha}(t,t) &= \left\langle\phi(h_\mu(t)) \phi(h_\alpha(t))\right\rangle_{\boldsymbol{\chi}, \xi} \\
            G^1_{\mu\alpha}(t,t) &= \left\langle g_\mu(t) g_\alpha(t)\right\rangle_{\boldsymbol{\chi}, \xi} \\
            \frac{\partial f_\mu}{\partial t} &= \sum_\alpha \left(K_{\mu\alpha}^x G^1_{\mu\alpha}(t,t) + \Phi^1_{\mu\alpha}(t,t)\right) \Delta_\alpha (t) \\
        \end{align}
        \right.
        $$
    </p>
    <p>
        数値計算する場合は、まずガウス分布から $\boldsymbol{\chi}_\mu, \xi$ を多数サンプリングする。
        次にそれぞれのサンプルを使って $h_\mu(t), g_\mu(t), z(t)$ の時間発展を好きな $T$ まで計算する。
        続いて各時刻で $\Phi^1_{\mu\alpha}(t,t), G^1_{\mu\alpha}(t,t)$ のサンプル平均を求める。
        最後に一番下の式を使って出力の時間発展を計算する。
    </p>


    <details>
        <summary>通常のDMFTとの違い（クリックして表示）</summary>
        <p>
            <a href="https://arxiv.org/abs/1809.06042">Crisanti & Sompolinsky</a>とか<a href="https://doi.org/10.48550/arXiv.1901.10416">Helias & Dahmen</a>
            で紹介されたような通常のDMFTでは、相互作用する多数の微視変数で書いたモーメント生成関数を変形して、巨視変数である平均場との相互作用の独立な積に帰着させるという方法が用いられていた。
            それに対して、今回の方法では、微視変数 $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ の時点で独立な1体問題の積に分解できている。
            モーメント生成関数を導入したのは巨視変数であるカーネルを少ない自由度で記述するための見通しが良くなるからだ。
            この点で今回のDMFTの使い方は非典型的である。
        </p>
        <p>
            普通のDMFTと共通するのはモーメント生成関数(モーメント母関数)を導入して指数関数の肩でごちゃごちゃ計算するという点なので、<strong>「DMFTはお母さんの肩をもむ」</strong>という語呂合わせで覚えよう。
        </p>
        <p class="close-btn">(クリックして非表示)</p>
    </details>


    <h3>本題：一般のLの場合</h3>
    <details>
        <summary>
            巨視変数であるカーネルに変数変換して、（クリックして表示）
        </summary>
        <p>
            まず、今後のために次のように変形する。
            \begin{align}
                Z &= \exp\left(N \cdot \frac{1}{N} \ln Z\right) \\
                &= \exp\left(N \cdot \frac{1}{N} \sum_{i=1}^N \ln Z_1\right)
            \end{align}
        </p>
        <p>
            次に、$\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ に変数変換した時と同様に、$\delta$ 関数を使ってカーネルの定義式となる恒等式を導入する。
            \begin{align}
                1 &= N \int d\Phi^1_{\mu\alpha}(t,s) \delta\left(N \Phi^1_{\mu\alpha}(t,s) - \phi(\boldsymbol{h}_\mu(t)) \cdot \phi(\boldsymbol{h}_\alpha(s))\right) \\
                &= Ndtds \int \frac{d\Phi^1_{\mu\alpha}(t,s)d\hat\Phi^1_{\mu\alpha}(t,s)}{2\pi i} \exp\left(dtds\hat\Phi^1_{\mu\alpha}(t,s)\left(N\Phi^1_{\mu\alpha}(t,s) - \phi(\boldsymbol{h}_\mu(t)) \cdot \phi(\boldsymbol{h}_\alpha(s))\right)\right) \\
                1 &= N \int dG^1_{\mu\alpha}(t,s) \delta\left(N G^1_{\mu\alpha}(t,s) - \boldsymbol{g}_\mu(t)\cdot\boldsymbol{g}_\alpha(s)\right) \\
                &= Ndtds \int \frac{dG^1_{\mu\alpha}(t,s)d\hat{G}^1_{\mu\alpha}(t,s)}{2\pi i} \exp\left(dtds\hat{G}^1_{\mu\alpha}(t,s)\left(N G^1_{\mu\alpha}(t,s) - \boldsymbol{g}_\mu(t)\cdot\boldsymbol{g}_\alpha(s)\right)\right) 
            \end{align}
        </p>
        <p>
            最後に、全ての $\mu, \alpha, t,s$ に関する恒等式を、はじめの式の両辺に掛ける。
            \begin{align}
                Z[\{\boldsymbol{j}_\mu\}_{\mu \in [P]}, \boldsymbol{v}]
                &\propto \int \prod_{\mu, \alpha, t, s} d\Phi^1 d\hat\Phi^1 dG^1 d\hat{G}^1 \exp\left(NS[\Phi^1, \hat\Phi^1, G^1, \hat{G}^1]\right) \\
                S[\Phi^1, \hat\Phi^1, G^1, \hat{G}^1]
                &= \sum_{\mu, \alpha}\int dtds \left(
                    \hat\Phi^1 \Phi^1 + \hat{G}^1 G^1
                    - \frac{1}{N}\hat\Phi^1 \phi(\boldsymbol{h}_\mu(t)) \cdot \phi(\boldsymbol{h}_\alpha(s))
                    - \frac{1}{N}\hat{G}^1 \boldsymbol{g}_\mu(t) \cdot \boldsymbol{g}_\alpha(s)
                \right) \\
                &\qquad + \frac{1}{N} \sum_{i=1}^N \ln Z_1[\{j^\mu_i\}_{\mu \in [P]}, v_i] \\
                &= \sum_{\mu, \alpha}\int dtds \left( \hat\Phi^1 \Phi^1 + \hat{G}^1 G^1 \right) \\
                &\qquad + \frac{1}{N} \sum_{i=1}^N \ln \left(
                    Z_1[\{j^\mu_i\}_{\mu \in [P]}, v_i]
                    \exp\left(
                        -\sum_{\mu, \alpha}\int dtds \left(
                            \hat\Phi^1 \phi(h^\mu_i(t)) \phi(h^\alpha_i(s))
                            + \hat{G}^1 g^\mu_i(t) \cdot g^\alpha_i(s)
                        \right)
                    \right)
                \right)
            \end{align}
            ただし例えば $\Phi^1 = \Phi^1_{\mu\alpha}(t,s)$ と略記している。
            これでベクトル $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ とカーネル $\Phi^1_{\mu\alpha}(t,s), G^1_{\mu\alpha}(t,s)$ が独立に扱えるようになった。
        </p>
        <details>
            <summary>通常のDMFTとの違い（クリックして表示）</summary>
            <p>
                <a href hoge>Sompolinsky</a>とか<a href hoge>Helias & Dahmen</a>で紹介されたような通常のDMFTでは、相互作用する多数の微視変数で書いたモーメント生成関数を変形して、巨視変数である平均場との相互作用の独立な積に帰着させるという方法が用いられていた。
                それに対して、今回の方法では、微視変数 $\boldsymbol{\chi}_\mu, \boldsymbol{\xi}$ の時点で独立な1体問題の積に分解できている。
                しかし出力のダイナミクスを巨視変数で記述したいという要請があるので、無理やりモーメント生成関数を変数変換してカーネルが登場する形にしている。
                この点で今回のDMFTの使い方は非典型的である。
            </p>
            <p>
                普通のDMFTと共通するのはモーメント生成関数(モーメント母関数)を導入して指数関数の肩でごちゃごちゃ計算するという点なので、<strong>「DMFTはお母さんの肩をもむ」</strong>というスローガンで覚えよう。
            </p>
            <p class="close-btn">(クリックして非表示)</p>
        </details>
        <p class="close-btn">(クリックして非表示)</p>
    </details>

    <details>
        <summary>
            作用 $S$ について鞍点方程式を求めると、次の表式を得る。（クリックして表示）
        </summary>
        <p>
            ここでいう鞍点法とは、ある汎函数が $y = \int dx \exp(Ng(x))$ のように表されている時の、$N\to\infty$ での近似法である。
            $N$ が小さい時は広い範囲の $x$ が積分に寄与するが、$N$ が大きくなると $g(x)$ の最大値を与える $x$ の寄与が圧倒的になり、他の部分の寄与は無視できる。
            したがって $y \sim \exp(Ng(x^*))$ と近似できる。ただし $x^*$ は $g(x)$ の最大値を与える $x$ であり、$\frac{dg(x)}{dx}|_{x=x^*} = 0$ を満たす。
            定数倍はズレるけど今回は気にしなくてOK。
        </p>
        <p>
            今の状況は、$Z \propto \int d\Phi^1 d\hat\Phi^1 dG^1 d\hat{G}^1 \exp\left(NS[\Phi^1, \hat\Phi^1, G^1, \hat{G}^1]\right)$ と表せていて、
            $S$ が $O_N(1)$ の大きさを持つので、鞍点法が使える。具体的な鞍点方程式は以下の通り。
            \begin{align}
                \frac{\delta S}{\delta \Phi^1_{\mu\alpha}(t,s)} &= 0 \to \hat\Phi^1_{\mu\alpha}(t,s) = 0 \\
                \frac{\delta S}{\delta \hat\Phi^1_{\mu\alpha}(t,s)} &= 0 \to \Phi^1_{\mu\alpha}(t,s) = 0 \\
                \frac{\delta S}{\delta G^1_{\mu\alpha}(t,s)} &= 0 \to \hat{G}^1_{\mu\alpha}(t,s) = 0 \\
                \frac{\delta S}{\delta \hat{G}^1_{\mu\alpha}(t,s)} &= 0 \to G^1_{\mu\alpha}(t,s) = 0
            \end{align}
        </p>
        <p>
            これを解くと、</p>
        <p>
            余談だけど、鞍点法というより極値法と呼んだ方が実態に即していると思う。
            どうして鞍点法と呼ぶかは分からない。
        </p>
        <p class="close-btn">(クリックして非表示)</p>
    </details>

    <h3>元論文の誤植</h3>
    <p>ページ数はarxivのv3のPDFファイルに準拠している。</p>
    <table border="1">
        <tr>
            <th>ページ</th>
            <th>行</th>
            <th>誤</th>
            <th>正</th>
        </tr>
        <tr>
            <td>3</td>
            <td>式(2)と(3)の間</td>
            <td>$K_{\mu\alpha}^{NTK}(t,s)=\nabla_\theta f_\mu \cdot \nabla_\theta f_\alpha$</td>
            <td>$K_{\mu\alpha}^{NTK}(t,s)=\gamma^2 \nabla_\theta f_\mu \cdot \nabla_\theta f_\alpha$</td>
        </tr>
        <tr>
            <td>22</td>
            <td>式(20)</td>
            <td>$\Delta_\alpha(s)$</td>
            <td>$\Delta_\mu(s)$</td>
        </tr>
        <tr>
            <td>22</td>
            <td>式(21)の1行目</td>
            <td>$\boldsymbol{g}_\nu^{l+1}(t)$</td>
            <td>$\boldsymbol{g}_\mu^{l+1}(s)$</td>
        </tr>
        <tr>
            <td>22</td>
            <td>式(22)</td>
            <td>$\boldsymbol{\chi}_\mu^l(t)$</td>
            <td>$\boldsymbol{\chi}_\nu^{l+1}(t)$</td>
        </tr>
        <tr>
            <td>23</td>
            <td>式(25)の1行目</td>
            <td>$\frac{1}{\sqrt{N}}$</td>
            <td>$\frac{1}{\sqrt{D}}$</td>
        </tr>
        <tr>
            <td>23</td>
            <td>式(26)</td>
            <td>$d\boldsymbol{\chi}d\hat{\boldsymbol{\chi}}$</td>
            <td>$d\boldsymbol{\chi}_\mu d\hat{\boldsymbol{\chi}}_\mu$</td>
        </tr>
        <tr>
            <td>23</td>
            <td>式(27)の1行目</td>
            <td>$\exp\left(N\hat{\Phi}_{\mu\alpha}(t,s)\dots\right)$</td>
            <td>$\exp\left(\hat{\Phi}_{\mu\alpha}(t,s)\dots\right)$</td>
        </tr>
        <tr>
            <td>23</td>
            <td>式(27)の3行目</td>
            <td>$\exp\left(N\hat{G}_{\mu\alpha}(t,s)\dots\right)$</td>
            <td>$\exp\left(\hat{G}_{\mu\alpha}(t,s)\dots\right)$</td>
        </tr>
    </table>

  
    <div style="margin-top: 60px;"></div>
    <a href="../topics.html" target="_blank">記事一覧に戻る</a>
  </main>

  <a href="#" class="pageup" id="js-pageup" aria-label="ページトップへ戻る">
    <div class="arrow"></div>
  </a>

  <footer>
    <small>© 2025 Asahi Nakamuta. Designed by <a href="https://utsusemi.hiroec.com" target="_blank" rel="noopener">Utsusemi</a>.</small>
  </footer>

  <script src="../js/common.js"></script>
</body>

</html>